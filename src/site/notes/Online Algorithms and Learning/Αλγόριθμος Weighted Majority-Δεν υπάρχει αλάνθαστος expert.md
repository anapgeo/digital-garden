---
{"dg-publish":true,"permalink":"/online-algorithms-and-learning/algorithmos-weighted-majority-den-yparxei-alanthastos-expert/","created":"2025-03-25T14:58:23.087+02:00","updated":"2025-03-25T14:59:28.322+02:00"}
---


Ας θεωρήσουμε $\mathcal{Y}=\{0,1\}$ και $L(\hat{y},y)=|\hat{y}-y|$(δηλαδή classification πρόβλημα). Δεν θεωρούμε πως υπάρχει κάποιος αλάνθαστος expert. Η υπόθεση αυτή είναι πιο ρεαλιστική. 

Στο Halving αλγόριθμο χρησιμοποιώντας πλειοψηφική ψήφο απορρίπταμε για πάντα τους μισούς expert. Αυτό δεν συμβαίνει στον Weighted Majority αλγόριθμο στον οποίο θεωρούμε κάποια βάρη στην πλειοψηφική ψήφο:

- Οι πιο επιτυχημένοι ως τώρα experts θα έχουν μεγαλύτερο βάρος
- Αρχικά όλοι οι experts έχουν το ίδιο βάρος
- Ύστερα από κάποιο σφάλμα, θα πολλαπλασιάζουμε το βάρος του κάθε expert με μια παράμετρο $β \in [0,1]$

> Σημείωση: Ο Weighted Majority αλγόριθμος είναι γενίκευση του Halving ($β=0$)


### Ο αλγόριθμος 

![Screenshot_8 1.png](/img/user/Online%20Algorithms%20and%20Learning/Screenshot_8%201.png)


> Σημείωση: Πρακτικά για κάθε γύρο προβλέψεων λαμβάνουμε τα δεδομένα $x_t$. Στη συνέχεια κάνουμε μια πρόβλεψη με βάση το ελέγχουμε τα βάρη των N experts, συγκεκριμένα αν το άθροισμα των βαρών των experts που προβλέπουν 1 είναι μεγαλύτερο από το άθροισμα των βαρών των experts που προβλέπουν 0 τότε επιλέγουμε 1. Διαφορετικά προβλέπουμε 0.  Στη συνέχεια, μετά από κάθε πρόβλεψη, αν η πρόβλεψη που κάναμε ήταν λάθος τότε πολλαπλασιάζουμε τα βάρη των experts που έκαναν λάθος με ένα παράγοντα $β\in [0,1]$ 


Υπάρχει κάποιο άνω φράγμα για τα σφάλματα που κάνει ο αλγόριθμος αυτός;

>Θεώρημα. Έστω $β\in [0,1]$.  Έστω $m_T$ το πλήθος σφαλμάτων του Weighted Majority και έστω $m_T^*$ το πλήθος σφαλμάτων του καλύτερου expert. Τότε: $m_T\leq \frac{\log_2 N + m_T^*\log\frac{1}{\beta}}{\log\frac{2}{1+\beta}}$

Απόδειξη 

Ορίζουμε μια συνάρτηση δυναμικού: $W_t= \sum_{i=1}^{N} w_{it}$

Ο αλγόριθμος προβλέπει χρησιμοποιώντας πλειοψηφική βεβαρυμένη(weighted majority) ψήφο 

Άρα κάθε φορά που η πρόβλεψη είναι λανθασμένη, πολλαπλασιάζουμε με β τα βάρη όσων πλειοψήφισαν:

$W_{t+1}= \sum_{i:y_{it}=y_t}w_{it} + \sum_{i:y_{it}\neq y_t}\beta w_{it} \leq \frac{1+\beta}{2}W_t$

Γνωρίζουμε στην πρώτη επανάληψη $W_1=N$ επομένως ο τελικός αριθμός σφαλματων $m_T$ είναι: $W_T\leq (\frac{1+\beta}{2})^{m_T}\cdot N$

Έστω τώρα $m_{iT}$ o αριθμός των σφαλμάτων του expert $i$ στον γύρο $Τ$ και $w_{iT}$ το βάρος του. Τότε:

- $w_{iT}=\beta^{m_T^*}\leq W_T$
- άρα $\beta^{m_T^*} \leq (\frac{\beta + 1}{2})^{m_T}\cdot N$ και λογαριθμίζοντας παίρνουμε το ζητούμενο αποτέλεσμα



> Σημείωση: Είναι αξιοσημείωτο ότι δεν χρειαζόμαστε καμία υπόθεση για την είσοδο. Επίσης ισχύει $m_T\leq O(\log N)$ που είναι το ίδιο φράγμα με τον [[Online Algorithms and Learning/Αλγόριθμος Halving-Αλάνθαστος expert\|Αλγόριθμος Halving-Αλάνθαστος expert]] 



